とりあえず
https://docs.langchain.com/oss/python/langchain/rag
の続きをやることにする。
tool callingを行うRAGともう一つ、dynamic_promptというのもあるらしい。

```python
from langchain.agents.middleware import dynamic_prompt, ModelRequest

@dynamic_prompt
def prompt_with_context(request: ModelRequest) -> str:
    """Inject context into state messages."""
    last_query = request.state["messages"][-1].text
    retrieved_docs = vector_store.similarity_search(last_query)

    docs_content = "\n\n".join(doc.page_content for doc in retrieved_docs)

    system_message = (
        "You are a helpful assistant. Use the following context in your response:"
        f"\n\n{docs_content}"
    )

    return system_message


agent = create_agent(model, tools=[], middleware=[prompt_with_context])
```
システムプロンプトを状況に応じて変更したりできる
おんなじユーザとAIがスレッドで長く会話とかする時に、ツールの検索結果とかを入れ込めば会話履歴が汚れないからトークン節約になるかも
```bash
================================ Human Message =================================

What is task decomposition?
{'messages': [HumanMessage(content='What is task decomposition?', additional_kwargs={}, response_metadata={}, id='f08a2810-1e17-47a1-a959-507f23e12c7d'), AIMessage(content='Task decomposition is a project management technique that involves breaking down a complex task or project into smaller, more manageable sub-tasks. By dividing a task into smaller components, it becomes easier to assign responsibilities, track progress, and ensure that all necessary steps are completed successfully. Task decomposition helps team members focus on specific elements of a task, improves efficiency, and reduces the risk of overlooking important details. This approach can also help in estimating timelines, setting priorities, and allocating resources effectively.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 95, 'prompt_tokens': 30, 'total_tokens': 125, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'id': 'chatcmpl-DDpFnYspHmlCNzaONRlFEKgaaPf5n', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c9e9d-9bbe-7ad3-b587-6b4f746f0db5-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 30, 'output_tokens': 95, 'total_tokens': 125, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}
================================== Ai Message ==================================

Task decomposition is a project management technique that involves breaking down a complex task or project into smaller, more manageable sub-tasks. By dividing a task into smaller components, it becomes easier to assign responsibilities, track progress, and ensure that all necessary steps are completed successfully. Task decomposition helps team members focus on specific elements of a task, improves efficiency, and reduces the risk of overlooking important details. This approach can also help in estimating timelines, setting priorities, and allocating resources effectively.
```
単純にプロンプトを埋め込むミドルウェアを定義しているだけ。なんか便利そう。
その下に、AgentMiddleWareを利用したRaw Dataの保存スクリプトが書いてある。

使ってみたところ、
```python
取得ドキュメント数: 4

--- Document 1 ---
  metadata : {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 1638}
  content  : Component One: Planning#
A complicated task usually involves many steps. An agent needs to know what they are and plan ahead.
Task Decomposition#
Chain of thought (CoT; Wei et al. 2022) has become a s...

--- Document 2 ---
  metadata : {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 2578}
  content  : Task decomposition can be done (1) by LLM with simple prompting like "Steps for XYZ.\n1.", "What are the subgoals for achieving XYZ?", (2) by using task-specific instructions; e.g. "Write a story outl...

--- Document 3 ---
  metadata : {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 17734}
  content  : The AI assistant can parse user input to several tasks: [{"task": task, "id", task_id, "dep": dependency_task_ids, "args": {"text": text, "image": URL, "audio": URL, "video": URL}}]. The "dep" field d...

--- Document 4 ---
  metadata : {'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'start_index': 19303}
  content  : (3) Task execution: Expert models execute on the specific tasks and log results.
Instruction:

With the input and the inference results, the AI assistant needs to describe the process and results. The...
```
という感じでしっかりとメタデータの保存に成功している
dynamic_promptも
```python
@overload
def dynamic_prompt(
    func: _CallableReturningSystemMessage[StateT, ContextT],
) -> AgentMiddleware[StateT, ContextT]: ...
```
という仕様になっているので、結局のところAgentMiddleWareクラスを使うとlangchainのミドルウェアをいじれそう

```python
 middleware: A sequence of middleware instances to apply to the agent.

            Middleware can intercept and modify agent behavior at various stages.

            !!! tip ""

                See the [Middleware](https://docs.langchain.com/oss/python/langchain/middleware)
                docs for more information.
```
https://docs.langchain.com/oss/python/langchain/middleware/overview
に色々な詳細あり
![middlewareの中身](image.png)
こんな感じで、toolやresultを呼び出す前後に関数をかますことができるらしい。
https://docs.langchain.com/oss/python/langchain/middleware/built-in

この辺にいっぱいあるな
AntigravityだとかCodexのtool call時の承認機能とかはHumanIntheLoopとかを使えば利用できるわけだね
個人情報の検出とかもあるし、サブエージェントの追加もできる。

Streamは一旦置いておいて、Short-term memoryをやってみたい
```python
from langchain.agents import create_agent, AgentState
from langgraph.checkpoint.memory import InMemorySaver


class CustomAgentState(AgentState):  
    user_id: str
    preferences: dict

agent = create_agent(
    "gpt-5",
    tools=[get_user_info],
    state_schema=CustomAgentState,  
    checkpointer=InMemorySaver(),
)

# Custom state can be passed in invoke
result = agent.invoke(
    {
        "messages": [{"role": "user", "content": "Hello"}],
        "user_id": "user_123",  
        "preferences": {"theme": "dark"}  
    },
    {"configurable": {"thread_id": "1"}})
```

短期記憶を入れると、コンテキストウインドウがデカくなりすぎるので、色々対策する
```python
@before_model
def trim_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:
    """Keep only the last few messages to fit context window."""
    messages = state["messages"]

    if len(messages) <= 3:
        return None  # No changes needed

    first_msg = messages[1]
    recent_messages = messages[-3:] if len(messages) % 2 == 0 else messages[-4:]
    new_messages = [first_msg] + recent_messages

    return {
        "messages": [
            RemoveMessage(id=REMOVE_ALL_MESSAGES),
            *new_messages
        ]
    }
```
これのbefore_modelっていうのは、ミドルウェアの図のやつのことだと思う
```bash
I don’t know your name—you haven’t told me yet. What would you like me to call you?
```
ちょっと中身をいじってみたら、教えたはずの名前を忘れたので、うまく行ってそう。今まで直接注入していたが、これを使った方が綺麗に色々管理できる
要約も可能らしい
```python
SummarizationMiddleware(
  self,
  model: str | BaseChatModel,
  *,
  trigger: ContextSize | list[ContextSize] | None = None,
  keep: ContextSize = ('messages', _DEFAULT_MESSAGES_TO_KEEP),
  token_counter: TokenCounter = count_tokens_approximately,
  summary_prompt: str = DEFAULT_SUMMARY_PROMPT,
  trim_tokens_to_summarize: int | None = _DEFAULT_TRIM_TOKEN_LIMIT,
  **deprecated_kwargs: Any = {}
)
```
ほんとにSummerizationしてるか知りたいな
```bash
--- Step 1 ---
(要約なし)

--- Step 2 ---
(要約なし)

--- Step 3 の要約 ---
Here is a summary of the conversation to date:

## SESSION INTENT
The user introduced themselves and shared their interests.

## SUMMARY
The user greeted with "こんにちは" and shared their name as "太郎." They mentioned their hobbies: programming and mountain climbing.

## ARTIFACTS
None

## NEXT STEPS
Wait for the user's next input or request to continue assisting.

--- Step 4 の要約 ---
Here is a summary of the conversation to date:

## SESSION INTENT
The user’s primary goal is to learn detailed information about Python, specifically the mechanism and workings of decorators.

## SUMMARY
The user initially introduced themselves as "太郎" and shared interests in programming and mountain climbing. The conversation then shifted to a focused request where the user asked for a detailed explanation about Python, emphasizing understanding how decorators work. The assistant acknowledged the user’s interests and prompted further discussion, and then the user clearly requested in Japanese to learn about Python decorators.

## ARTIFACTS
None

## NEXT STEPS
Provide a clear, detailed explanation of Python decorators including their purpose, how they are defined, how they modify functions, and examples of common use cases. Clarify any related concepts such as closures or higher-order functions if relevant.

--- Step 5 の要約 ---
Here is a summary of the conversation to date:

## SESSION INTENT
The user’s primary goal is to learn detailed information about Python concepts, focusing first on decorators and now requesting an explanation about Python generators including how they differ from iterators.

## SUMMARY
- The user initially asked for a detailed explanation about Python decorators.
- The assistant explained decorators thoroughly, including concepts such as:
  - Decorators as functions wrapping other functions.
  - Syntax using the @decorator notation.
  - Inner workings using closures and higher-order functions.
  - Examples with and without function arguments.
  - Mention of built-in decorators like @staticmethod, @classmethod, @property, and functools.wraps.
- The user expressed gratitude and then requested a detailed explanation of Python generators, including the difference between generators and iterators.

## ARTIFACTS
None

## NEXT STEPS
Provide a clear, detailed explanation of Python generators, how they work, including the `yield` keyword. Explain iterators as well, clarifying the differences and relationships between iterators and generators in Python, with examples for better understanding.
```
ちゃんと要約されていることを確認した。
```python
 for step, user_msg in enumerate(conversations, 1):
        agent.invoke({"messages": user_msg}, config)

        # チェックポイントから要約メッセージだけ抽出して表示
        state = agent.get_state(config)
        messages = state.values.get("messages", [])
        summary = None
        for msg in messages:
            if "summary" in msg.content.lower():
                summary = msg.content
                break

        if summary:
            print(f"\n--- Step {step} の要約 ---\n{summary}")
        else:
            print(f"\n--- Step {step} ---\n(要約なし)")
```
コードはこれ。
結局諸々Dict型で管理されているらしい。
`test_summerization`の実装だと、トークンサイズが500に達した時点で要約を始めるらしい。
```python
trigger: One or more thresholds that trigger summarization.

                Provide a single
                [`ContextSize`][langchain.agents.middleware.summarization.ContextSize]
                tuple or a list of tuples, in which case summarization runs when any
                threshold is met.

                !!! example

                    ```python
                    # Trigger summarization when 50 messages is reached
                    ("messages", 50)

                    # Trigger summarization when 3000 tokens is reached
                    ("tokens", 3000)

                    # Trigger summarization either when 80% of model's max input tokens
                    # is reached or when 100 messages is reached (whichever comes first)
                    [("fraction", 0.8), ("messages", 100)]
                    ```

                    See [`ContextSize`][langchain.agents.middleware.summarization.ContextSize]
                    for more details.
```
トリガーは複数設定することが可能。必要なのかな...
結局のところコンテキストウィンドウを節約するための仕組みで、要約されたメッセージ+直前のいくつかの会話(keep)
を保持する？
```python
@override
    def before_model(
        self, state: AgentState[Any], runtime: Runtime[ContextT]
    ) -> dict[str, Any] | None:
        """Process messages before model invocation, potentially triggering summarization.

        Args:
            state: The agent state.
            runtime: The runtime environment.

        Returns:
            An updated state with summarized messages if summarization was performed.
        """
        messages = state["messages"]
        self._ensure_message_ids(messages)

        total_tokens = self.token_counter(messages)
        if not self._should_summarize(messages, total_tokens):
            return None

        cutoff_index = self._determine_cutoff_index(messages)

        if cutoff_index <= 0:
            return None

        messages_to_summarize, preserved_messages = self._partition_messages(messages, cutoff_index)

        summary = self._create_summary(messages_to_summarize)
        new_messages = self._build_new_messages(summary)

        return {
            "messages": [
                RemoveMessage(id=REMOVE_ALL_MESSAGES),
                *new_messages,
                *preserved_messages,
            ]
        }
```
`SummarizationMiddleware`も結局`AgentMiddleWare`の継承クラスで、`before_model`をオーバーライドしたカスタムクラス。
メッセージ自体を丸々書き換える感じで実装されている。return Noneって何やねんって感じだが、少なくともデコレータの方のbefore_modelは、Noneが返却された場合 no updateという意味になるらしいので、これも同様だろうみたいな
ところで、`InMemorySaver`が何をしているかよくわかんない。どうやら`langgraph`のノードとして登録されるみたいだが...

